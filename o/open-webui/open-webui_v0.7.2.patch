diff --git a/Dockerfile b/Dockerfile
index ac81a3943..5c8f64cb0 100644
--- a/Dockerfile
+++ b/Dockerfile
@@ -1,6 +1,7 @@
 # syntax=docker/dockerfile:1
 # Initialize device type args
 # use build args in the docker build command with --build-arg="BUILDARG=true"
+# Arguments
 ARG USE_CUDA=false
 ARG USE_OLLAMA=false
 ARG USE_SLIM=false
@@ -23,8 +24,8 @@ ARG BUILD_HASH=dev-build
 ARG UID=0
 ARG GID=0
 
-######## WebUI frontend ########
-FROM --platform=$BUILDPLATFORM node:22-alpine3.20 AS build
+######## STAGE 1 : WebUI frontend ########
+FROM --platform=$BUILDPLATFORM node:22 AS build
 ARG BUILD_HASH
 
 # Set Node.js options (heap limit Allocation failed - JavaScript heap out of memory)
@@ -32,19 +33,80 @@ ARG BUILD_HASH
 
 WORKDIR /app
 
+#Node.js Dependencies and Build
 # to store git revision in build
-RUN apk add --no-cache git
-
+RUN apt update && apt install git -y
 COPY package.json package-lock.json ./
-RUN npm ci --force
+RUN rm -rf package-lock.json && npm install --force
 
+#Copy Source and Build
 COPY . .
 ENV APP_BUILD_HASH=${BUILD_HASH}
 RUN npm run build
 
-######## WebUI backend ########
+####### STAGE 2 : Install dependencies #######
+FROM python:3.11.14-slim-bookworm AS deps
+#Use args for conditional installs
+ARG USE_CUDA
+ARG USE_CUDA_VER
+ARG USE_SLIM
+ARG USE_EMBEDDING_MODEL
+
+WORKDIR /app/backend
+
+#Install common system dependencies needed for building Python Packages
+RUN apt-get update && \
+    apt-get install -y --no-install-recommends \
+        git build-essential pandoc gcc python3-dev netcat-openbsd curl jq \
+        ffmpeg libsm6 libxext6 \
+        && rm -rf /var/lib/apt/lists/*
+
+#Python Packages setup
+RUN pip3 install --no-cache-dir uv
+
+#Copy only requirements files
+COPY ./backend/requirements.txt ./requirements.txt
+
+#RUN python -m pip install --no-cache-dir -r requirements.txt
+
+
+#COPY ./backend/requirements.txt ./requirements.txt
+#RUN python -m pip install --no-cache-dir -r requirements.txt
+
+#Dependency install and Model pre-download (if not slim)
+ENV RAG_EMBEDDING_MODEL=$USE_EMBEDDING_MODEL \
+        WHISPER_MODEL="base" \
+    WHISPER_MODEL_DIR="/app/backend/data/cache/whisper/models" \
+    TICKTOKEN_ENCODING_NAME="cl100k_base"
+
+RUN if [ "$USE_CUDA" = "true" ]; then \
+#CUDA build : Install PyTorch with CUDA support
+#Predownload models on CUDA build
+pip3 install uvicorn --no-cache-dir && \
+    pip3 install fastapi typer pydantic sqlalchemy aiocache aiohttp anyio requests redis starlette_compress itsdangerous fastapi starsessions loguru opentelemetry-api cryptography markdown bs4 asgiref jwt bcrypt pytz --no-cache-dir && \
+    pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/$USE_CUDA_VER --no-cache-dir && \
+    uv pip install --system -r requirements.txt --no-cache-dir && \
+    python -c "import os; from sentence_transformers import SentenceTransformer; SentenceTransformer(os.environ['RAG_EMBEDDING_MODEL'], device='cpu')" && \
+    python -c "import os; from faster_whisper import WhisperModel; WhisperModel(os.environ['WHISPER_MODEL'], device='cpu', compute_type='int8', download_root=os.environ['WHISPER_MODEL_DIR'])"; \
+    python -c "import os; import tiktoken; tiktoken.get_encoding(os.environ['TIKTOKEN_ENCODING_NAME'])"; \
+    else \
+    pip3 install torch torchvision torchaudio --no-cache-dir && \
+    python -m pip install -r requirements.txt --no-cache-dir && \
+    if [ "$USE_SLIM" != "true" ]; then \
+    python -c "import os; from sentence_transformers import SentenceTransformer; SentenceTransformer(os.environ['RAG_EMBEDDING_MODEL'], device='cpu')" && \
+    python -c "import os; from faster_whisper import WhisperModel; WhisperModel(os.environ['WHISPER_MODEL'], device='cpu', compute_type='int8', download_root=os.environ['WHISPER_MODEL_DIR'])"; \
+    python -c "import os; import tiktoken; tiktoken.get_encoding(os.environ['TIKTOKEN_ENCODING_NAME'])"; \
+    fi; \
+    fi; \
+    mkdir -p /app/backend/data && chown -R $UID:$GID /app/backend/data/ && \
+    rm -rf /var/lib/apt/lists/*;
+
+######## STAGE 3 : WebUI backend ########
 FROM python:3.11.14-slim-bookworm AS base
 
+ENV UV_EXTRA_INDEX_URL=https://repo.fury.io/mgiessing
+ENV PIP_EXTRA_INDEX_URL=https://repo.fury.io/mgiessing
+
 # Use args
 ARG USE_CUDA
 ARG USE_OLLAMA
@@ -60,9 +122,14 @@ ARG GID
 # Python settings
 ENV PYTHONUNBUFFERED=1
 
-## Basis ##
+WORKDIR /app/backend
+ENV HOME=/root
+
+## Environment Variables for Runtime Configuration Basis ##
 ENV ENV=prod \
     PORT=8080 \
+    DOCKER=true \
+    WEBUI_BUILD_VERSION=${BUILD_HASH} \
     # pass build args to the build
     USE_OLLAMA_DOCKER=${USE_OLLAMA} \
     USE_CUDA_DOCKER=${USE_CUDA} \
@@ -104,12 +171,17 @@ ENV HF_HOME="/app/backend/data/cache/embedding/models"
 ## Torch Extensions ##
 # ENV TORCH_EXTENSIONS_DIR="/.cache/torch_extensions"
 
-#### Other models ##########################################################
 
-WORKDIR /app/backend
+# Install common system dependencies
+RUN apt-get update && \
+    apt-get install -y --no-install-recommends \
+    git build-essential pandoc gcc netcat-openbsd curl jq \
+    python3-dev \
+    ffmpeg libsm6 libxext6 \
+    && rm -rf /var/lib/apt/lists/*
 
-ENV HOME=/root
-# Create user and group if not root
+# User Setup and Permissions
+# Create non-root user/group
 RUN if [ $UID -ne 0 ]; then \
     if [ $GID -ne 0 ]; then \
     addgroup --gid $GID app; \
@@ -117,45 +189,15 @@ RUN if [ $UID -ne 0 ]; then \
     adduser --uid $UID --gid $GID --home $HOME --disabled-password --no-create-home app; \
     fi
 
+#Copy only requirements files
+COPY ./backend/requirements.txt ./requirements.txt
+RUN python -m pip install --no-cache-dir -r requirements.txt
+
 RUN mkdir -p $HOME/.cache/chroma
 RUN echo -n 00000000-0000-0000-0000-000000000000 > $HOME/.cache/chroma/telemetry_user_id
-
 # Make sure the user has access to the app and root directory
 RUN chown -R $UID:$GID /app $HOME
 
-# Install common system dependencies
-RUN apt-get update && \
-    apt-get install -y --no-install-recommends \
-    git build-essential pandoc gcc netcat-openbsd curl jq \
-    python3-dev \
-    ffmpeg libsm6 libxext6 \
-    && rm -rf /var/lib/apt/lists/*
-
-# install python dependencies
-COPY --chown=$UID:$GID ./backend/requirements.txt ./requirements.txt
-
-RUN pip3 install --no-cache-dir uv && \
-    if [ "$USE_CUDA" = "true" ]; then \
-    # If you use CUDA the whisper and embedding model will be downloaded on first use
-    pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/$USE_CUDA_DOCKER_VER --no-cache-dir && \
-    uv pip install --system -r requirements.txt --no-cache-dir && \
-    python -c "import os; from sentence_transformers import SentenceTransformer; SentenceTransformer(os.environ['RAG_EMBEDDING_MODEL'], device='cpu')" && \
-    python -c "import os; from sentence_transformers import SentenceTransformer; SentenceTransformer(os.environ.get('AUXILIARY_EMBEDDING_MODEL', 'TaylorAI/bge-micro-v2'), device='cpu')" && \
-    python -c "import os; from faster_whisper import WhisperModel; WhisperModel(os.environ['WHISPER_MODEL'], device='cpu', compute_type='int8', download_root=os.environ['WHISPER_MODEL_DIR'])"; \
-    python -c "import os; import tiktoken; tiktoken.get_encoding(os.environ['TIKTOKEN_ENCODING_NAME'])"; \
-    else \
-    pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu --no-cache-dir && \
-    uv pip install --system -r requirements.txt --no-cache-dir && \
-    if [ "$USE_SLIM" != "true" ]; then \
-    python -c "import os; from sentence_transformers import SentenceTransformer; SentenceTransformer(os.environ['RAG_EMBEDDING_MODEL'], device='cpu')" && \
-    python -c "import os; from sentence_transformers import SentenceTransformer; SentenceTransformer(os.environ.get('AUXILIARY_EMBEDDING_MODEL', 'TaylorAI/bge-micro-v2'), device='cpu')" && \
-    python -c "import os; from faster_whisper import WhisperModel; WhisperModel(os.environ['WHISPER_MODEL'], device='cpu', compute_type='int8', download_root=os.environ['WHISPER_MODEL_DIR'])"; \
-    python -c "import os; import tiktoken; tiktoken.get_encoding(os.environ['TIKTOKEN_ENCODING_NAME'])"; \
-    fi; \
-    fi; \
-    mkdir -p /app/backend/data && chown -R $UID:$GID /app/backend/data/ && \
-    rm -rf /var/lib/apt/lists/*;
-
 # Install Ollama if requested
 RUN if [ "$USE_OLLAMA" = "true" ]; then \
     date +%s > /tmp/ollama_build_hash && \
@@ -164,6 +206,13 @@ RUN if [ "$USE_OLLAMA" = "true" ]; then \
     rm -rf /var/lib/apt/lists/*; \
     fi
 
+## Copy artifacts from build stages
+# copy Python environment from 'deps' stages
+COPY --from=deps /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages
+#COPY --from=deps /usr/local/lib/uv /usr/local/lib/uv
+COPY --chown=$UID:$GID --from=deps /app/backend/data /app/backend/data
+
+
 # copy embedding weight from build
 # RUN mkdir -p /root/.cache/chroma/onnx_models/all-MiniLM-L6-v2
 # COPY --from=build /app/onnx /root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx
@@ -178,6 +227,7 @@ COPY --chown=$UID:$GID ./backend .
 
 EXPOSE 8080
 
+#Healthcheck and Final Permissisons
 HEALTHCHECK CMD curl --silent --fail http://localhost:${PORT:-8080}/health | jq -ne 'input.status == true' || exit 1
 
 # Minimal, atomic permission hardening for OpenShift (arbitrary UID):
@@ -193,8 +243,4 @@ RUN if [ "$USE_PERMISSION_HARDENING" = "true" ]; then \
 
 USER $UID:$GID
 
-ARG BUILD_HASH
-ENV WEBUI_BUILD_VERSION=${BUILD_HASH}
-ENV DOCKER=true
-
 CMD [ "bash", "start.sh"]
diff --git a/backend/requirements.txt b/backend/requirements.txt
index 51f0a8a1a..0a1ffe568 100644
--- a/backend/requirements.txt
+++ b/backend/requirements.txt
@@ -119,7 +119,7 @@ boto3==1.42.21
 
 pymilvus==2.6.6
 qdrant-client==1.16.2
-playwright==1.57.0 # Caution: version must match docker-compose.playwright.yaml - Update the docker-compose.yaml if necessary
+#playwright==1.57.0 # Caution: version must match docker-compose.playwright.yaml - Update the docker-compose.yaml if necessary
 elasticsearch==9.2.1
 pinecone==6.0.2
 oracledb==3.4.1
diff --git a/package.json b/package.json
index bc71db855..25cd76b38 100644
--- a/package.json
+++ b/package.json
@@ -150,5 +150,10 @@
 	"engines": {
 		"node": ">=18.13.0 <=22.x.x",
 		"npm": ">=6.0.0"
-	}
+	},
+        "overrides": {
+                "@parcel/watcher": "npm:@parcel/watcher-wasm",
+                "lightningcss": "npm:lightningcss-wasm",
+                "@tailwindcss/oxide": "https://npm.fury.io/mgiessing/~/up/ver_1hoo34/%40tailwindcss%2Foxide-4.1.13.tgz"
+        }
 }
