From 70ebe2084057eeaf427493ce5a35a33f525efcb1 Mon Sep 17 00:00:00 2001
From: Nishidha Panpaliya <nishidha.panpaliya@partner.ibm.com>
Date: Thu, 4 Sep 2025 08:56:34 -0400
Subject: [PATCH] Excluded source that has commercial license

Signed-off-by: Nishidha Panpaliya <nishidha.panpaliya@partner.ibm.com>
---
 README.md                                   |   2 -
 setup.py                                    |   2 +-
 src/torchaudio/pipelines/__init__.py        |  37 +----
 src/torchaudio/pipelines/_squim_pipeline.py | 156 --------------------
 4 files changed, 2 insertions(+), 195 deletions(-)
 delete mode 100644 src/torchaudio/pipelines/_squim_pipeline.py

diff --git a/README.md b/README.md
index 6494094e..66213813 100644
--- a/README.md
+++ b/README.md
@@ -80,6 +80,4 @@ Pre-trained Model License
 
 The pre-trained models provided in this library may have their own licenses or terms and conditions derived from the dataset used for training. It is your responsibility to determine whether you have permission to use the models for your use case.
 
-For instance, SquimSubjective model is released under the Creative Commons Attribution Non Commercial 4.0 International (CC-BY-NC 4.0) license. See [the link](https://zenodo.org/record/4660670#.ZBtWPOxuerN) for additional details.
-
 Other pre-trained models that have different license are noted in documentation. Please checkout the [documentation page](https://pytorch.org/audio/main/).
diff --git a/setup.py b/setup.py
index 7eb94528..e3823429 100644
--- a/setup.py
+++ b/setup.py
@@ -128,7 +128,7 @@ def _main():
             "Topic :: Multimedia :: Sound/Audio",
             "Topic :: Scientific/Engineering :: Artificial Intelligence",
         ],
-        packages=find_packages(where="src"),
+        packages=find_packages(where="src", exclude=["torchaudio.pipelines._wav2vec2", "torchaudio.pipelines._wav2vec2.*"]),
         package_dir={"": "src"},
         ext_modules=setup_helpers.get_ext_modules(),
         cmdclass={
diff --git a/src/torchaudio/pipelines/__init__.py b/src/torchaudio/pipelines/__init__.py
index efec1f35..4d26b274 100644
--- a/src/torchaudio/pipelines/__init__.py
+++ b/src/torchaudio/pipelines/__init__.py
@@ -4,7 +4,7 @@ from ._source_separation_pipeline import (
     HDEMUCS_HIGH_MUSDB_PLUS,
     SourceSeparationBundle,
 )
-from ._squim_pipeline import SQUIM_OBJECTIVE, SQUIM_SUBJECTIVE, SquimObjectiveBundle, SquimSubjectiveBundle
+#from ._squim_pipeline import SQUIM_OBJECTIVE, SQUIM_SUBJECTIVE, SquimObjectiveBundle, SquimSubjectiveBundle
 from ._tts import (
     TACOTRON2_GRIFFINLIM_CHAR_LJSPEECH,
     TACOTRON2_GRIFFINLIM_PHONE_LJSPEECH,
@@ -12,41 +12,6 @@ from ._tts import (
     TACOTRON2_WAVERNN_PHONE_LJSPEECH,
     Tacotron2TTSBundle,
 )
-from ._wav2vec2.impl import (
-    HUBERT_ASR_LARGE,
-    HUBERT_ASR_XLARGE,
-    HUBERT_BASE,
-    HUBERT_LARGE,
-    HUBERT_XLARGE,
-    MMS_FA,
-    VOXPOPULI_ASR_BASE_10K_DE,
-    VOXPOPULI_ASR_BASE_10K_EN,
-    VOXPOPULI_ASR_BASE_10K_ES,
-    VOXPOPULI_ASR_BASE_10K_FR,
-    VOXPOPULI_ASR_BASE_10K_IT,
-    WAV2VEC2_ASR_BASE_100H,
-    WAV2VEC2_ASR_BASE_10M,
-    WAV2VEC2_ASR_BASE_960H,
-    WAV2VEC2_ASR_LARGE_100H,
-    WAV2VEC2_ASR_LARGE_10M,
-    WAV2VEC2_ASR_LARGE_960H,
-    WAV2VEC2_ASR_LARGE_LV60K_100H,
-    WAV2VEC2_ASR_LARGE_LV60K_10M,
-    WAV2VEC2_ASR_LARGE_LV60K_960H,
-    WAV2VEC2_BASE,
-    WAV2VEC2_LARGE,
-    WAV2VEC2_LARGE_LV60K,
-    WAV2VEC2_XLSR53,
-    WAV2VEC2_XLSR_1B,
-    WAV2VEC2_XLSR_2B,
-    WAV2VEC2_XLSR_300M,
-    Wav2Vec2ASRBundle,
-    Wav2Vec2Bundle,
-    Wav2Vec2FABundle,
-    WAVLM_BASE,
-    WAVLM_BASE_PLUS,
-    WAVLM_LARGE,
-)
 from .rnnt_pipeline import EMFORMER_RNNT_BASE_LIBRISPEECH, RNNTBundle
 
 
diff --git a/src/torchaudio/pipelines/_squim_pipeline.py b/src/torchaudio/pipelines/_squim_pipeline.py
deleted file mode 100644
index 0c70db4a..00000000
--- a/src/torchaudio/pipelines/_squim_pipeline.py
+++ /dev/null
@@ -1,156 +0,0 @@
-from dataclasses import dataclass
-
-import torch
-import torchaudio
-
-from torchaudio.models import squim_objective_base, squim_subjective_base, SquimObjective, SquimSubjective
-
-
-@dataclass
-class SquimObjectiveBundle:
-    """Data class that bundles associated information to use pretrained
-    :py:class:`~torchaudio.models.SquimObjective` model.
-
-    This class provides interfaces for instantiating the pretrained model along with
-    the information necessary to retrieve pretrained weights and additional data
-    to be used with the model.
-
-    Torchaudio library instantiates objects of this class, each of which represents
-    a different pretrained model. Client code should access pretrained models via these
-    instances.
-
-    This bundle can estimate objective metric scores for speech enhancement, such as STOI, PESQ, Si-SDR.
-    A typical use case would be a flow like `waveform -> list of scores`. Please see below for the code example.
-
-    Example: Estimate the objective metric scores for the input waveform.
-        >>> import torch
-        >>> import torchaudio
-        >>> from torchaudio.pipelines import SQUIM_OBJECTIVE as bundle
-        >>>
-        >>> # Load the SquimObjective bundle
-        >>> model = bundle.get_model()
-        Downloading: "https://download.pytorch.org/torchaudio/models/squim_objective_dns2020.pth"
-        100%|████████████| 28.2M/28.2M [00:03<00:00, 9.24MB/s]
-        >>>
-        >>> # Resample audio to the expected sampling rate
-        >>> waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)
-        >>>
-        >>> # Estimate objective metric scores
-        >>> scores = model(waveform)
-        >>> print(f"STOI: {scores[0].item()},  PESQ: {scores[1].item()}, SI-SDR: {scores[2].item()}.")
-    """  # noqa: E501
-
-    _path: str
-    _sample_rate: float
-
-    def get_model(self) -> SquimObjective:
-        """Construct the SquimObjective model, and load the pretrained weight.
-
-        Returns:
-            Variation of :py:class:`~torchaudio.models.SquimObjective`.
-        """
-        model = squim_objective_base()
-        path = torchaudio.utils.download_asset(f"models/{self._path}")
-        state_dict = torch.load(path, weights_only=True)
-        model.load_state_dict(state_dict)
-        model.eval()
-        return model
-
-    @property
-    def sample_rate(self):
-        """Sample rate of the audio that the model is trained on.
-
-        :type: float
-        """
-        return self._sample_rate
-
-
-SQUIM_OBJECTIVE = SquimObjectiveBundle(
-    "squim_objective_dns2020.pth",
-    _sample_rate=16000,
-)
-SQUIM_OBJECTIVE.__doc__ = """SquimObjective pipeline trained using approach described in
-    :cite:`kumar2023torchaudio` on the *DNS 2020 Dataset* :cite:`reddy2020interspeech`.
-
-    The underlying model is constructed by :py:func:`torchaudio.models.squim_objective_base`.
-    The weights are under `Creative Commons Attribution 4.0 International License
-    <https://github.com/microsoft/DNS-Challenge/blob/interspeech2020/master/LICENSE>`__.
-
-    Please refer to :py:class:`SquimObjectiveBundle` for usage instructions.
-    """
-
-
-@dataclass
-class SquimSubjectiveBundle:
-    """Data class that bundles associated information to use pretrained
-    :py:class:`~torchaudio.models.SquimSubjective` model.
-
-    This class provides interfaces for instantiating the pretrained model along with
-    the information necessary to retrieve pretrained weights and additional data
-    to be used with the model.
-
-    Torchaudio library instantiates objects of this class, each of which represents
-    a different pretrained model. Client code should access pretrained models via these
-    instances.
-
-    This bundle can estimate subjective metric scores for speech enhancement, such as MOS.
-    A typical use case would be a flow like `waveform -> score`. Please see below for the code example.
-
-    Example: Estimate the subjective metric scores for the input waveform.
-        >>> import torch
-        >>> import torchaudio
-        >>> from torchaudio.pipelines import SQUIM_SUBJECTIVE as bundle
-        >>>
-        >>> # Load the SquimSubjective bundle
-        >>> model = bundle.get_model()
-        Downloading: "https://download.pytorch.org/torchaudio/models/squim_subjective_bvcc_daps.pth"
-        100%|████████████| 360M/360M [00:09<00:00, 41.1MB/s]
-        >>>
-        >>> # Resample audio to the expected sampling rate
-        >>> waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)
-        >>> # Use a clean reference (doesn't need to be the reference for the waveform) as the second input
-        >>> reference = torchaudio.functional.resample(reference, sample_rate, bundle.sample_rate)
-        >>>
-        >>> # Estimate subjective metric scores
-        >>> score = model(waveform, reference)
-        >>> print(f"MOS: {score}.")
-    """  # noqa: E501
-
-    _path: str
-    _sample_rate: float
-
-    def get_model(self) -> SquimSubjective:
-        """Construct the SquimSubjective model, and load the pretrained weight.
-        Returns:
-            Variation of :py:class:`~torchaudio.models.SquimObjective`.
-        """
-        model = squim_subjective_base()
-        path = torchaudio.utils.download_asset(f"models/{self._path}")
-        state_dict = torch.load(path, weights_only=True)
-        model.load_state_dict(state_dict)
-        model.eval()
-        return model
-
-    @property
-    def sample_rate(self):
-        """Sample rate of the audio that the model is trained on.
-
-        :type: float
-        """
-        return self._sample_rate
-
-
-SQUIM_SUBJECTIVE = SquimSubjectiveBundle(
-    "squim_subjective_bvcc_daps.pth",
-    _sample_rate=16000,
-)
-SQUIM_SUBJECTIVE.__doc__ = """SquimSubjective pipeline trained
-    as described in :cite:`manocha2022speech` and :cite:`kumar2023torchaudio`
-    on the *BVCC* :cite:`cooper2021voices` and *DAPS* :cite:`mysore2014can` datasets.
-
-    The underlying model is constructed by :py:func:`torchaudio.models.squim_subjective_base`.
-    The weights are under `Creative Commons Attribution Non Commercial 4.0 International
-    <https://zenodo.org/record/4660670#.ZBtWPOxuerN>`__.
-
-    Please refer to :py:class:`SquimSubjectiveBundle` for usage instructions.
-    """
-- 
2.47.3

