diff --git a/README.md b/README.md
index af2b0216..34cf6999 100644
--- a/README.md
+++ b/README.md
@@ -88,6 +88,4 @@ Pre-trained Model License

 The pre-trained models provided in this library may have their own licenses or terms and conditions derived from the dataset used for training. It is your responsibility to determine whether you have permission to use the models for your use case.

-For instance, SquimSubjective model is released under the Creative Commons Attribution Non Commercial 4.0 International (CC-BY-NC 4.0) license. See [the link](https://zenodo.org/record/4660670#.ZBtWPOxuerN) for additional details.
-
 Other pre-trained models that have different license are noted in documentation. Please checkout the [documentation page](https://pytorch.org/audio/main/).
diff --git a/setup.py b/setup.py
index 7a2fc746..9904ad2d 100644
--- a/setup.py
+++ b/setup.py
@@ -128,7 +128,7 @@ def _main():
             "Topic :: Multimedia :: Sound/Audio",
             "Topic :: Scientific/Engineering :: Artificial Intelligence",
         ],
-        packages=find_packages(where="src"),
+        packages=find_packages(where="src", exclude=["torchaudio.pipelines._wav2vec2", "torchaudio.pipelines._wav2vec2.*"]),
         package_dir={"": "src"},
         ext_modules=setup_helpers.get_ext_modules(),
         cmdclass={
diff --git a/src/torchaudio/pipelines/__init__.py b/src/torchaudio/pipelines/__init__.py
index efec1f35..4d26b274 100644
--- a/src/torchaudio/pipelines/__init__.py
+++ b/src/torchaudio/pipelines/__init__.py
@@ -4,7 +4,7 @@ from ._source_separation_pipeline import (
     HDEMUCS_HIGH_MUSDB_PLUS,
     SourceSeparationBundle,
 )
-from ._squim_pipeline import SQUIM_OBJECTIVE, SQUIM_SUBJECTIVE, SquimObjectiveBundle, SquimSubjectiveBundle
+#from ._squim_pipeline import SQUIM_OBJECTIVE, SQUIM_SUBJECTIVE, SquimObjectiveBundle, SquimSubjectiveBundle
 from ._tts import (
     TACOTRON2_GRIFFINLIM_CHAR_LJSPEECH,
     TACOTRON2_GRIFFINLIM_PHONE_LJSPEECH,
@@ -12,41 +12,6 @@ from ._tts import (
     TACOTRON2_WAVERNN_PHONE_LJSPEECH,
     Tacotron2TTSBundle,
 )
-from ._wav2vec2.impl import (
-    HUBERT_ASR_LARGE,
-    HUBERT_ASR_XLARGE,
-    HUBERT_BASE,
-    HUBERT_LARGE,
-    HUBERT_XLARGE,
-    MMS_FA,
-    VOXPOPULI_ASR_BASE_10K_DE,
-    VOXPOPULI_ASR_BASE_10K_EN,
-    VOXPOPULI_ASR_BASE_10K_ES,
-    VOXPOPULI_ASR_BASE_10K_FR,
-    VOXPOPULI_ASR_BASE_10K_IT,
-    WAV2VEC2_ASR_BASE_100H,
-    WAV2VEC2_ASR_BASE_10M,
-    WAV2VEC2_ASR_BASE_960H,
-    WAV2VEC2_ASR_LARGE_100H,
-    WAV2VEC2_ASR_LARGE_10M,
-    WAV2VEC2_ASR_LARGE_960H,
-    WAV2VEC2_ASR_LARGE_LV60K_100H,
-    WAV2VEC2_ASR_LARGE_LV60K_10M,
-    WAV2VEC2_ASR_LARGE_LV60K_960H,
-    WAV2VEC2_BASE,
-    WAV2VEC2_LARGE,
-    WAV2VEC2_LARGE_LV60K,
-    WAV2VEC2_XLSR53,
-    WAV2VEC2_XLSR_1B,
-    WAV2VEC2_XLSR_2B,
-    WAV2VEC2_XLSR_300M,
-    Wav2Vec2ASRBundle,
-    Wav2Vec2Bundle,
-    Wav2Vec2FABundle,
-    WAVLM_BASE,
-    WAVLM_BASE_PLUS,
-    WAVLM_LARGE,
-)
 from .rnnt_pipeline import EMFORMER_RNNT_BASE_LIBRISPEECH, RNNTBundle


diff --git a/src/torchaudio/pipelines/_squim_pipeline.py b/src/torchaudio/pipelines/_squim_pipeline.py
deleted file mode 100644
index f7e7c1d9..00000000
--- a/src/torchaudio/pipelines/_squim_pipeline.py
+++ /dev/null
@@ -1,156 +0,0 @@
-from dataclasses import dataclass
-
-import torch
-import torchaudio
-
-from torchaudio.models import squim_objective_base, squim_subjective_base, SquimObjective, SquimSubjective
-
-
-@dataclass
-class SquimObjectiveBundle:
-    """Data class that bundles associated information to use pretrained
-    :py:class:`~torchaudio.models.SquimObjective` model.
-
-    This class provides interfaces for instantiating the pretrained model along with
-    the information necessary to retrieve pretrained weights and additional data
-    to be used with the model.
-
-    Torchaudio library instantiates objects of this class, each of which represents
-    a different pretrained model. Client code should access pretrained models via these
-    instances.
-
-    This bundle can estimate objective metric scores for speech enhancement, such as STOI, PESQ, Si-SDR.
-    A typical use case would be a flow like `waveform -> list of scores`. Please see below for the code example.
-
-    Example: Estimate the objective metric scores for the input waveform.
-        >>> import torch
-        >>> import torchaudio
-        >>> from torchaudio.pipelines import SQUIM_OBJECTIVE as bundle
-        >>>
-        >>> # Load the SquimObjective bundle
-        >>> model = bundle.get_model()
-        Downloading: "https://download.pytorch.org/torchaudio/models/squim_objective_dns2020.pth"
-        100%|████████████| 28.2M/28.2M [00:03<00:00, 9.24MB/s]
-        >>>
-        >>> # Resample audio to the expected sampling rate
-        >>> waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)
-        >>>
-        >>> # Estimate objective metric scores
-        >>> scores = model(waveform)
-        >>> print(f"STOI: {scores[0].item()},  PESQ: {scores[1].item()}, SI-SDR: {scores[2].item()}.")
-    """  # noqa: E501
-
-    _path: str
-    _sample_rate: float
-
-    def get_model(self) -> SquimObjective:
-        """Construct the SquimObjective model, and load the pretrained weight.
-
-        Returns:
-            Variation of :py:class:`~torchaudio.models.SquimObjective`.
-        """
-        model = squim_objective_base()
-        path = torchaudio.utils._download_asset(f"models/{self._path}")
-        state_dict = torch.load(path, weights_only=True)
-        model.load_state_dict(state_dict)
-        model.eval()
-        return model
-
-    @property
-    def sample_rate(self):
-        """Sample rate of the audio that the model is trained on.
-
-        :type: float
-        """
-        return self._sample_rate
-
-
-SQUIM_OBJECTIVE = SquimObjectiveBundle(
-    "squim_objective_dns2020.pth",
-    _sample_rate=16000,
-)
-SQUIM_OBJECTIVE.__doc__ = """SquimObjective pipeline trained using approach described in
-    :cite:`kumar2023torchaudio` on the *DNS 2020 Dataset* :cite:`reddy2020interspeech`.
-
-    The underlying model is constructed by :py:func:`torchaudio.models.squim_objective_base`.
-    The weights are under `Creative Commons Attribution 4.0 International License
-    <https://github.com/microsoft/DNS-Challenge/blob/interspeech2020/master/LICENSE>`__.
-
-    Please refer to :py:class:`SquimObjectiveBundle` for usage instructions.
-    """
-
-
-@dataclass
-class SquimSubjectiveBundle:
-    """Data class that bundles associated information to use pretrained
-    :py:class:`~torchaudio.models.SquimSubjective` model.
-
-    This class provides interfaces for instantiating the pretrained model along with
-    the information necessary to retrieve pretrained weights and additional data
-    to be used with the model.
-
-    Torchaudio library instantiates objects of this class, each of which represents
-    a different pretrained model. Client code should access pretrained models via these
-    instances.
-
-    This bundle can estimate subjective metric scores for speech enhancement, such as MOS.
-    A typical use case would be a flow like `waveform -> score`. Please see below for the code example.
-
-    Example: Estimate the subjective metric scores for the input waveform.
-        >>> import torch
-        >>> import torchaudio
-        >>> from torchaudio.pipelines import SQUIM_SUBJECTIVE as bundle
-        >>>
-        >>> # Load the SquimSubjective bundle
-        >>> model = bundle.get_model()
-        Downloading: "https://download.pytorch.org/torchaudio/models/squim_subjective_bvcc_daps.pth"
-        100%|████████████| 360M/360M [00:09<00:00, 41.1MB/s]
-        >>>
-        >>> # Resample audio to the expected sampling rate
-        >>> waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)
-        >>> # Use a clean reference (doesn't need to be the reference for the waveform) as the second input
-        >>> reference = torchaudio.functional.resample(reference, sample_rate, bundle.sample_rate)
-        >>>
-        >>> # Estimate subjective metric scores
-        >>> score = model(waveform, reference)
-        >>> print(f"MOS: {score}.")
-    """  # noqa: E501
-
-    _path: str
-    _sample_rate: float
-
-    def get_model(self) -> SquimSubjective:
-        """Construct the SquimSubjective model, and load the pretrained weight.
-        Returns:
-            Variation of :py:class:`~torchaudio.models.SquimObjective`.
-        """
-        model = squim_subjective_base()
-        path = torchaudio.utils._download_asset(f"models/{self._path}")
-        state_dict = torch.load(path, weights_only=True)
-        model.load_state_dict(state_dict)
-        model.eval()
-        return model
-
-    @property
-    def sample_rate(self):
-        """Sample rate of the audio that the model is trained on.
-
-        :type: float
-        """
-        return self._sample_rate
-
-
-SQUIM_SUBJECTIVE = SquimSubjectiveBundle(
-    "squim_subjective_bvcc_daps.pth",
-    _sample_rate=16000,
-)
-SQUIM_SUBJECTIVE.__doc__ = """SquimSubjective pipeline trained
-    as described in :cite:`manocha2022speech` and :cite:`kumar2023torchaudio`
-    on the *BVCC* :cite:`cooper2021voices` and *DAPS* :cite:`mysore2014can` datasets.
-
-    The underlying model is constructed by :py:func:`torchaudio.models.squim_subjective_base`.
-    The weights are under `Creative Commons Attribution Non Commercial 4.0 International
-    <https://zenodo.org/record/4660670#.ZBtWPOxuerN>`__.
-
-    Please refer to :py:class:`SquimSubjectiveBundle` for usage instructions.
-    """
