os: linux
dist: focal
arch: ppc64le
language: shell

branches:
  only:
    - stripe-python

env:
  global:
    - ENABLE_TRIVY=true
    - ENABLE_SYFT=true
    - ENABLE_GRYPE=true

services:
  - docker

before_install:
    - sudo apt update -y
    - sudo apt install -y jq; jq --version
   

jobs:
  include:
    - stage: Build info
      name: Get Build info details
      script:
        # This script will read build_info.sh and create variable.sh file with export statement, so that we can source this file in later stages to use variables.
        - chmod +x ./script/read_buildinfo.sh;bash ./script/read_buildinfo.sh
      workspaces:
        create:
          #need to cache file variable.sh using workspaces by the name build_cache
          name: build_cache
          paths:
            - variable.sh

    - stage: Build
      if: env(VALIDATE_BUILD_SCRIPT) = true
      name: Build script run
      script:
        - source variable.sh
        # creating a folder package-cache to store the variable.sh and cloned package folder
        - mkdir package-cache && sudo mv variable.sh package-cache
        - chmod +x ./script/build_package.sh; bash ./script/build_package.sh;
        # setting a variable cloned_package,the value of the variable is based on recently cloned package through build-script execution.
        - cloned_package=$(ls -td -- */ | head -n 1) && sudo mv "$cloned_package" package-cache
        - echo "export CLONED_PACKAGE=\"$cloned_package\"" >> package-cache/variable.sh
        - cd package-cache
        # changing the ownership of package-cache and it's sub directories to travis, the reason for doing that is to facilitate creation of files
        # in the cloned package folder(note: currently cloned package folder is owned by the root)
        - sudo chown travis:travis -R .
        - ls -ltr
        # executing pre-process script which creates some files based on the package language.(python, javascript, typescript)
        - chmod +x ../script/pre_process.sh; bash ../script/pre_process.sh;
        - echo $TRAVIS_BUILD_DIR
        - cd $TRAVIS_BUILD_DIR
        - gzip $TRAVIS_BUILD_DIR/build_log
        - chmod +x ./script/upload-scripts/upload_file.sh; bash ./script/upload-scripts/upload_file.sh build_log.gz
      workspaces:
        create:
          # package_cache contains both variable.sh file as well as cloned package directory.
          name: package_cache
          paths:
            - .
        use: build_cache

    - stage: Wheel build
      if: env(WHEEL_BUILD) = true
      name: "Create Wheel for Python 3.9"
      env: PYTHON_VERSION="3.9"
      script:
        - echo "wheel build python 3.9"
        - source variable.sh
        - chmod +x ./script/build_wheels.sh; bash ./script/build_wheels.sh;
        - sudo chmod a+r *.whl
        - WHEEL_FILE=*.whl
        - chmod +x ./script/upload-scripts/upload_wheel.sh; bash ./script/upload-scripts/upload_wheel.sh $WHEEL_FILE
      workspaces:
        create:
          name: wheels_cache_39
          paths:
            - ./*.whl
        use: build_cache
      allow_failure: true

    - name: "Create Wheel for Python 3.10"
      if: env(WHEEL_BUILD) = true
      env: PYTHON_VERSION="3.10"
      script:
        - echo "wheel build python 3.10"
        - source variable.sh
        - chmod +x ./script/build_wheels.sh; bash ./script/build_wheels.sh;
        - sudo chmod a+r *.whl
        - WHEEL_FILE=*.whl
        - chmod +x ./script/upload-scripts/upload_wheel.sh; bash ./script/upload-scripts/upload_wheel.sh $WHEEL_FILE
      workspaces:
        create:
          name: wheels_cache_310
          paths:
            - ./*.whl
        use: build_cache

    - name: "Create Wheel for Python 3.11"
      if: env(WHEEL_BUILD) = true
      env: PYTHON_VERSION="3.11"
      script:
        - echo "wheel build python 3.11"
        - source variable.sh
        - chmod +x ./script/build_wheels.sh; bash ./script/build_wheels.sh;
        - sudo chmod a+r *.whl
        - WHEEL_FILE=*.whl
        - chmod +x ./script/upload-scripts/upload_wheel.sh; bash ./script/upload-scripts/upload_wheel.sh $WHEEL_FILE
      workspaces:
        create:
          name: wheels_cache_311
          paths: ./*.whl
        use: build_cache

    - name: "Create Wheel for Python 3.12"
      if: env(WHEEL_BUILD) = true
      env: PYTHON_VERSION="3.12"
      script:
        - echo "wheel build python 3.12"
        - source variable.sh
        - chmod +x ./script/build_wheels.sh; bash ./script/build_wheels.sh;
        - sudo chmod a+r *.whl
        - WHEEL_FILE=*.whl
        - chmod +x ./script/upload-scripts/upload_wheel.sh; bash ./script/upload-scripts/upload_wheel.sh $WHEEL_FILE
      workspaces:
        create:
          name: wheels_cache_312
          paths:
            - ./*.whl
        use: build_cache

    - name: "Create Wheel for Python 3.13"
      if: env(WHEEL_BUILD) = true
      env: PYTHON_VERSION="3.13"
      script:
        - echo "wheel build python 3.13"
        - source variable.sh
        - chmod +x ./script/build_wheels.sh; bash ./script/build_wheels.sh;
        - sudo chmod a+r *.whl
        - WHEEL_FILE=*.whl
        - chmod +x ./script/upload-scripts/upload_wheel.sh; bash ./script/upload-scripts/upload_wheel.sh $WHEEL_FILE
      workspaces:
        create:
          name: wheels_cache_313
          paths:
            - ./*.whl
        use: build_cache
      allow_failure: true

    - stage: Source code scanner
      if: env(ENABLE_TRIVY) = true AND env(VALIDATE_BUILD_SCRIPT) = true
      name: Run trivy scan
      script:
        - ls -ltr package-cache
        - source package-cache/variable.sh;
        - chmod +x ./script/scanner-scripts/trivy_code_scan.sh;bash ./script/scanner-scripts/trivy_code_scan.sh;
        - mkdir source_trivy
        - mv package-cache/trivy_source_vulnerabilities_results.json package-cache/trivy_source_sbom_results.cyclonedx source_trivy
      workspaces:
        create:
          name: source_trivy
          paths:
            - source_trivy
        use:
          - package_cache

    - name: Run syft scan
      if: env(ENABLE_SYFT) = true AND env(VALIDATE_BUILD_SCRIPT) = true
      script:
        - source package-cache/variable.sh;
        - chmod +x ./script/scanner-scripts/syft_code_scan.sh;bash ./script/scanner-scripts/syft_code_scan.sh;
        - mkdir source_syft
        - mv package-cache/syft_source_sbom_results.json source_syft
      workspaces:
        create:
          name: source_syft
          paths:
            - source_syft
        use:
          - package_cache

    - name: Run grype scan
      if: env(ENABLE_GRYPE) = true AND env(VALIDATE_BUILD_SCRIPT) = true
      script:
        - source package-cache/variable.sh;
        - chmod +x ./script/scanner-scripts/grype_code_scan.sh;bash ./script/scanner-scripts/grype_code_scan.sh;
        - mkdir source_grype
        - mv package-cache/grype_source_sbom_results.json package-cache/grype_source_vulnerabilities_results.json source_grype
      workspaces:
        create:
          name: source_grype
          paths:
            - source_grype
        use:
          - package_cache

    - stage: Upload source scanner results
      if: env(VALIDATE_BUILD_SCRIPT) = true
      name: source scanner results
      workspaces:
        use:
          - source_trivy
          - source_syft
          - source_grype
      script:
        - mkdir source
        # the below commands will copy the files from all the scanner folders to source folder,
        #later we tar that source folder and use the upload scanner script to push into cos bucket.
        - find source_trivy -mindepth 1 -exec mv -t source {} + 2>/dev/null || true
        - find source_syft -mindepth 1 -exec mv -t source {} + 2>/dev/null || true
        - find source_grype -mindepth 1 -exec mv -t source {} + 2>/dev/null || true
        - tar cvzf source_scanner.tar.gz source
        - chmod +x ./script/upload-scripts/upload_file.sh; bash ./script/upload-scripts/upload_file.sh source_scanner.tar.gz

    - stage: Build docker image
      # the below condition checks if docker_build flag set to true or false, it it is true it will build this stage and if it is false it will exclude this stage.
      if: env(BUILD_DOCKER) = true
      name: Docker build
      script:
        - source variable.sh;
        - chmod +x ./script/build_docker.sh; bash ./script/build_docker.sh;echo "printing docker images"; docker images;
        # the below command is used to upload docker image to ibm icr registry.
        - chmod +x ./script/upload-scripts/upload_docker_image.sh; bash ./script/upload-scripts/upload_docker_image.sh;
      workspaces:
        create:
          name: docker_image_tar
          paths:
            - image.tar
        use: build_cache

    - stage: Docker image Scanner
      if: env(BUILD_DOCKER) = true AND env(ENABLE_TRIVY) = true
      name: Run trivy scan
      script:
        - source variable.sh;
        - docker load -i "$HOME/build/$TRAVIS_REPO_SLUG/image.tar"; chmod +x ./script/scanner-scripts/trivy_image_scan.sh;bash ./script/scanner-scripts/trivy_image_scan.sh; mkdir image_trivy; mv trivy_image_vulnerabilities_results.json trivy_image_sbom_results.cyclonedx image_trivy;
      workspaces:
        create:
          name: image_trivy
          paths:
            - image_trivy
        use:
          - build_cache
          - docker_image_tar

    - name: Run syft scan
      if: env(BUILD_DOCKER) = true AND env(ENABLE_SYFT) = true
      script:
        - source variable.sh;
        - docker load -i "$HOME/build/$TRAVIS_REPO_SLUG/image.tar"; chmod +x ./script/scanner-scripts/syft_image_scan.sh;bash ./script/scanner-scripts/syft_image_scan.sh; mkdir image_syft; mv syft_image_sbom_results.json image_syft;
      workspaces:
        create:
          name: image_syft
          paths:
            - image_syft
        use:
          - build_cache
          - docker_image_tar

    - name: Run grype scan
      if: env(BUILD_DOCKER) = true AND env(ENABLE_GRYPE) = true
      script:
        - source variable.sh;
        - docker load -i "$HOME/build/$TRAVIS_REPO_SLUG/image.tar"; chmod +x ./script/scanner-scripts/grype_image_scan.sh;bash ./script/scanner-scripts/grype_image_scan.sh; mkdir image_grype; mv grype_image_sbom_results.json grype_image_vulnerabilities_results.json image_grype;
      workspaces:
        create:
          name: image_grype
          paths:
            - image_grype
        use:
          - build_cache
          - docker_image_tar

    - stage: Upload image scanner results
      if: env(BUILD_DOCKER) = true
      name: image scanner results
      workspaces:
        use:
          - image_trivy
          - image_syft
          - image_grype
          - build_cache
      script:
        - source variable.sh;
        # the below command will copy the files from all the scanner folders to image folder, later we tar that image folder and use the upload scanner script to push into cos bucket.
        - mkdir image
        - find image_trivy -mindepth 1 -exec mv -t image {} + 2>/dev/null || true
        - find image_syft -mindepth 1 -exec mv -t image {} + 2>/dev/null || true
        - find image_grype -mindepth 1 -exec mv -t image {} + 2>/dev/null || true
        - tar cvzf image_scanner.tar.gz image
        - chmod +x ./script/upload-scripts/upload_file.sh; bash ./script/upload-scripts/upload_file.sh image_scanner.tar.gz
