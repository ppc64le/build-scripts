From 333cb42e9a1546dd2d59c7162416af1b45e7dfa3 Mon Sep 17 00:00:00 2001
From: Rushikesh Sathe <Rushikesh.Sathe@ibm.com>
Date: Thu, 26 Jun 2025 06:46:41 +0000
Subject: [PATCH] fix for CVE-2025-3730

---
 aten/src/ATen/native/LossCTC.cpp             |  1 +
 aten/src/ATen/native/cpu/BinaryOpsKernel.cpp | 18 ++++++++++++++++++
 aten/src/ATen/native/cuda/LossCTC.cu         |  1 +
 cmake/Dependencies.cmake                     |  2 ++
 test/test_nn.py                              |  8 ++++++++
 tools/setup_helpers/cmake.py                 |  5 +++++
 6 files changed, 35 insertions(+)

diff --git a/aten/src/ATen/native/LossCTC.cpp b/aten/src/ATen/native/LossCTC.cpp
index 530f3cf066e..96c2cf86cd2 100644
--- a/aten/src/ATen/native/LossCTC.cpp
+++ b/aten/src/ATen/native/LossCTC.cpp
@@ -127,6 +127,7 @@ std::tuple<Tensor, Tensor, size_t, std::vector<int64_t>> ctc_loss_allocate_outpu
 // the alphas from the user by only returning the loss.
 template<typename scalar_t, ScalarType target_scalar_type>
 std::tuple<Tensor, Tensor> ctc_loss_cpu_template(const Tensor& log_probs, const Tensor& targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t BLANK) {
+  TORCH_CHECK(log_probs.numel() > 0, "log_probs tensor must not be empty");
   // log_probs: input_len x batch_size x num_labels
   // targets [int64]: batch_size x target_length OR sum(target_lengths)
   constexpr scalar_t neginf = -std::numeric_limits<scalar_t>::infinity();
diff --git a/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp b/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp
index 42a4d0b564b..009f77809f8 100644
--- a/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp
+++ b/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp
@@ -428,6 +428,14 @@ void bitwise_xor_kernel(TensorIteratorBase& iter) {
 }

 void lshift_kernel(TensorIteratorBase& iter) {
+#if defined(__VSX__)  || defined(CPU_CAPABILITY_VSX)
+  AT_DISPATCH_INTEGRAL_TYPES(iter.dtype(), "lshift_cpu", [&]() {
+    cpu_kernel(iter,
+      [](scalar_t a, scalar_t b) -> scalar_t {
+        return static_cast<std::make_unsigned_t<scalar_t>>(a) << b;
+    });
+ });
+#else
   AT_DISPATCH_INTEGRAL_TYPES(iter.dtype(), "lshift_cpu", [&]() {
     cpu_kernel_vec(
         iter,
@@ -441,6 +449,7 @@ void lshift_kernel(TensorIteratorBase& iter) {
         },
         [](Vectorized<scalar_t> a, Vectorized<scalar_t> b) { return a << b; });
   });
+#endif
 }

 void logical_and_kernel(TensorIterator& iter) {
@@ -499,6 +508,14 @@ void logical_xor_kernel(TensorIterator& iter) {
 }

 void rshift_kernel(TensorIteratorBase& iter) {
+#if defined(__VSX__)  || defined(CPU_CAPABILITY_VSX)
+  AT_DISPATCH_INTEGRAL_TYPES(iter.dtype(), "rshift_cpu", [&]() {
+    cpu_kernel(iter,
+      [](scalar_t a, scalar_t b) -> scalar_t {
+        return a >> b;
+      });
+  });
+#else
   AT_DISPATCH_INTEGRAL_TYPES(iter.dtype(), "rshift_cpu", [&]() {
     cpu_kernel_vec(
         iter,
@@ -515,6 +532,7 @@ void rshift_kernel(TensorIteratorBase& iter) {
         },
         [](Vectorized<scalar_t> a, Vectorized<scalar_t> b) { return a >> b; });
   });
+#endif
 }

 void lt_kernel(TensorIteratorBase& iter) {
diff --git a/aten/src/ATen/native/cuda/LossCTC.cu b/aten/src/ATen/native/cuda/LossCTC.cu
index 759955dc62f..5ec51970683 100644
--- a/aten/src/ATen/native/cuda/LossCTC.cu
+++ b/aten/src/ATen/native/cuda/LossCTC.cu
@@ -219,6 +219,7 @@ ctc_loss_log_alpha_gpu_kernel(scalar_t* __restrict__ log_alpha_data,
 // backward. The dispatch function will only return the loss.
 template<typename scalar_t, ScalarType target_scalar_type>
 std::tuple<Tensor, Tensor> ctc_loss_gpu_template(const Tensor& log_probs, const Tensor& targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t BLANK) {
+  TORCH_CHECK(log_probs.numel() > 0, "log_probs tensor must not be empty");
   // log_probs: input_len x batch_size x num_labels
   // targets [int64]: batch_size x target_length OR sum(target_lengths)
   CheckedFrom c = "ctc_loss_gpu";
diff --git a/cmake/Dependencies.cmake b/cmake/Dependencies.cmake
index 1813f4418a2..7b9a555cee3 100644
--- a/cmake/Dependencies.cmake
+++ b/cmake/Dependencies.cmake
@@ -1144,6 +1144,8 @@ if(USE_DISTRIBUTED AND USE_TENSORPIPE)
       set(TP_ENABLE_CUDA_IPC ON CACHE BOOL "" FORCE)
     endif()
     set(TP_BUILD_LIBUV ON CACHE BOOL "" FORCE)
+    set(TP_ENABLE_SHM OFF CACHE BOOL "" FORCE)
+    set(TP_ENABLE_CMA OFF CACHE BOOL "" FORCE)
     add_compile_options(-DTORCH_USE_LIBUV)
     include_directories(BEFORE SYSTEM ${CMAKE_CURRENT_LIST_DIR}/../third_party/tensorpipe/third_party/libuv/include)
     set(TP_STATIC_OR_SHARED STATIC CACHE STRING "" FORCE)
diff --git a/test/test_nn.py b/test/test_nn.py
index 0af76d427e2..7b6d327be51 100644
--- a/test/test_nn.py
+++ b/test/test_nn.py
@@ -11333,6 +11333,14 @@ class TestNNDeviceType(NNTestCase):
         self.assertTrue("Cudnn" in str(loss_cudnn.grad_fn))
         grad_cudnn, = torch.autograd.grad(loss_cudnn, log_probs, grad_out)
         self.assertEqual(grad_cudnn, grad_native, atol=1e-4, rtol=0)
+    @expectedFailureMPS
+    def test_ctc_loss_error(self, device):
+        log_probs = torch.rand(0, 0, 4, device=device)
+        targets = torch.tensor([], device=device, dtype=torch.long)Add commentMore actions
+        input_lengths = torch.tensor([], device=device, dtype=torch.long)
+        target_lengths = torch.tensor([], device=device, dtype=torch.long)
+        with self.assertRaisesRegex(RuntimeError, "log_probs tensor must not be empty"):
+            F.ctc_loss(log_probs, targets, input_lengths, target_lengths, reduction='none')

     @expectedFailureMPS  # RuntimeError: LSTM with projections is not currently supported with MPS.
     @dtypesIfCUDA(torch.half, torch.float, torch.double)
diff --git a/tools/setup_helpers/cmake.py b/tools/setup_helpers/cmake.py
index 84e4dad32d3..5768fa92cd5 100644
--- a/tools/setup_helpers/cmake.py
+++ b/tools/setup_helpers/cmake.py
@@ -190,6 +190,11 @@ class CMake:
             # Key: environment variable name. Value: Corresponding variable name to be passed to CMake. If you are
             # adding a new build option to this block: Consider making these two names identical and adding this option
             # in the block below.
+            "Protobuf_INCLUDE_DIR" : "Protobuf_INCLUDE_DIR",
+            "Protobuf_LIBRARIES" : "Protobuf_LIBRARIES",
+            "Protobuf_LIBRARY": "Protobuf_LIBRARY",
+            "Protobuf_LITE_LIBRARY" : "Protobuf_LITE_LIBRARY",
+            "Protobuf_PROTOC_EXECUTABLE": "Protobuf_PROTOC_EXECUTABLE",
             "_GLIBCXX_USE_CXX11_ABI": "GLIBCXX_USE_CXX11_ABI",
             "CUDNN_LIB_DIR": "CUDNN_LIBRARY",
             "USE_CUDA_STATIC_LINK": "CAFFE2_STATIC_LINK_CUDA",
--
2.47.1
